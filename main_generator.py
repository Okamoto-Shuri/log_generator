"""
Main Generator Module - Part 3-3 Integration

ãƒ¡ã‚¤ãƒ³ã®ãƒ­ã‚°ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
"""

import json
import random
import sys
from typing import List
from datetime import datetime, timedelta

# coreãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from core import (
    logger,
    GeneratorConfig,
    WeightNormalizer,
    HostStateManager,
    SemanticVectorGenerator,
    MetricsGenerator,
    LogFormatter,
    LogRecordFactory,
    LogRecord,
    TimeManager,
    StatisticsCollector,
    SCENARIO_META,
    initialize_generator
)

# scenariosãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from scenarios import (
    NormalScenarioGenerator,
    CompleteScenarioFactory
)


# ==================== ãƒ¡ã‚¤ãƒ³ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ ====================

class EnhancedLogGenerator:
    """æ”¹å–„ç‰ˆãƒ­ã‚°ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ï¼ˆv3.0ï¼‰"""
    
    def __init__(self, config: GeneratorConfig):
        """
        Args:
            config: ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿è¨­å®š
        """
        self.config = config
        self.stats = StatisticsCollector()
        
        # åˆæœŸåŒ–å‡¦ç†
        logger.info("Initializing Enhanced Log Generator v3.0...")
        initialize_generator(config)
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.host_state_manager = HostStateManager() if config.enable_host_state else None
        self.vector_generator = SemanticVectorGenerator(config.embedding_dim)
        self.metrics_generator = MetricsGenerator(self.host_state_manager)
        self.formatter = LogFormatter()
        self.record_factory = LogRecordFactory(
            config,
            self.vector_generator,
            self.metrics_generator
        )
        self.time_manager = TimeManager(
            datetime.now() - timedelta(days=config.start_time_days_ago)
        )
        
        # ã‚·ãƒŠãƒªã‚ªã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿
        self.normal_generator = NormalScenarioGenerator(
            self.record_factory,
            self.formatter
        )
        self.scenario_factory = CompleteScenarioFactory(
            self.record_factory,
            self.formatter
        )
        
        # é‡ã¿ã®æ­£è¦åŒ–
        normalizer = WeightNormalizer()
        self.normalized_weights = normalizer.normalize_weights(SCENARIO_META)
        
        logger.info("Initialization completed")
    
    def _prepare_event_schedule(self) -> List[str]:
        """
        ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’äº‹å‰ã«ä½œæˆ
        
        Returns:
            ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆï¼ˆ"normal" ã¾ãŸã¯ ã‚·ãƒŠãƒªã‚ªã‚³ãƒ¼ãƒ‰ï¼‰
        """
        logger.info("Preparing event schedule...")
        
        # ç•°å¸¸ãƒ»æ­£å¸¸ã®ä»¶æ•°æ±ºå®š
        abnormal_count = int(self.config.total_events * self.config.abnormal_ratio)
        normal_count = self.config.total_events - abnormal_count
        
        # ç•°å¸¸ã‚·ãƒŠãƒªã‚ªã®å‰²ã‚Šå½“ã¦ï¼ˆé‡ã¿ä»˜ãï¼‰
        scenarios = list(self.normalized_weights.keys())
        weights = list(self.normalized_weights.values())
        abnormal_events = random.choices(scenarios, weights=weights, k=abnormal_count)
        
        # å…¨ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¦ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        all_events = abnormal_events + ["normal"] * normal_count
        random.shuffle(all_events)
        
        logger.info(
            f"Schedule prepared: {normal_count} normal, "
            f"{abnormal_count} abnormal events"
        )
        
        return all_events
    
    def _generate_event(self, event_type: str) -> List[LogRecord]:
        """
        å˜ä¸€ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            event_type: "normal" ã¾ãŸã¯ ã‚·ãƒŠãƒªã‚ªã‚³ãƒ¼ãƒ‰
            
        Returns:
            ãƒ­ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
        """
        base_time = self.time_manager.get_current_time()
        
        try:
            if event_type == "normal":
                logs = self.normal_generator.generate(base_time)
            else:
                scenario = self.scenario_factory.create(event_type)
                logs = scenario.generate(base_time)
            
            # çµ±è¨ˆè¨˜éŒ²
            self.stats.record_scenario(event_type, len(logs))
            
            return logs
        
        except Exception as e:
            logger.error(f"Failed to generate event {event_type}: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ­£å¸¸ç³»ã‚’ç”Ÿæˆ
            logs = self.normal_generator.generate(base_time)
            self.stats.record_scenario("normal", len(logs))
            return logs
    
    def _write_batch(
        self,
        file_handle,
        batch: List[LogRecord]
    ) -> None:
        """
        ãƒãƒƒãƒã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        
        Args:
            file_handle: ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ«
            batch: ãƒ­ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ãƒãƒƒãƒ
        """
        if not batch:
            return
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆ
        batch.sort(key=lambda x: x.timestamp)
        
        # JSON Lineså½¢å¼ã§æ›¸ãè¾¼ã¿
        for record in batch:
            json_line = json.dumps(record.to_dict(), ensure_ascii=False)
            file_handle.write(json_line + "\n")
        
        logger.debug(f"Wrote batch of {len(batch)} records")
    
    def _save_partial_results(
        self,
        logs: List[LogRecord],
        reason: str = "interrupted"
    ) -> None:
        """
        éƒ¨åˆ†çš„ãªçµæœã‚’ä¿å­˜
        
        Args:
            logs: ä¿å­˜ã™ã‚‹ãƒ­ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰
            reason: ä¿å­˜ç†ç”±
        """
        if not logs:
            logger.warning("No logs to save")
            return
        
        partial_file = self.config.output_file.replace(
            ".jsonl",
            f"_partial_{reason}.jsonl"
        )
        
        try:
            with open(partial_file, "w", encoding="utf-8") as f:
                self._write_batch(f, logs)
            
            logger.info(
                f"Partial results ({len(logs)} logs) saved to {partial_file}"
            )
            print(f"\nâš ï¸  Partial results saved to: {partial_file}")
        
        except Exception as e:
            logger.error(f"Failed to save partial results: {e}")
    
    def run(self) -> None:
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯"""
        logger.info("Starting log generation...")
        self.stats.start_timing()
        
        print("\n" + "=" * 70)
        print(f"ğŸš€ Enhanced Log Generator v3.0")
        print("=" * 70)
        print(f"Total Events:     {self.config.total_events:,}")
        print(f"Abnormal Ratio:   {self.config.abnormal_ratio:.1%}")
        print(f"Batch Size:       {self.config.batch_size:,}")
        print(f"Output File:      {self.config.output_file}")
        print("=" * 70 + "\n")
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ
        event_schedule = self._prepare_event_schedule()
        
        # ãƒãƒƒãƒãƒãƒƒãƒ•ã‚¡
        batch_buffer: List[LogRecord] = []
        
        try:
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ï¼ˆtqdmãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            try:
                from tqdm import tqdm
                event_iterator = tqdm(
                    event_schedule,
                    desc="Generating logs",
                    unit="event"
                )
            except ImportError:
                event_iterator = event_schedule
                logger.info("Install tqdm for progress bar: pip install tqdm")
                print("â³ Generating logs (install tqdm for progress bar)...\n")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã„ã¦ãƒãƒƒãƒå‡¦ç†
            with open(self.config.output_file, "w", encoding="utf-8") as f:
                for event_type in event_iterator:
                    # æ™‚åˆ»ã‚’é€²ã‚ã‚‹
                    self.time_manager.advance()
                    
                    # ã‚¤ãƒ™ãƒ³ãƒˆç”Ÿæˆ
                    logs = self._generate_event(event_type)
                    batch_buffer.extend(logs)
                    
                    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰æ›¸ãè¾¼ã¿
                    if len(batch_buffer) >= self.config.batch_size:
                        self._write_batch(f, batch_buffer)
                        batch_buffer.clear()
                
                # æ®‹ã‚Šã®ãƒãƒƒãƒ•ã‚¡ã‚’æ›¸ãè¾¼ã¿
                if batch_buffer:
                    self._write_batch(f, batch_buffer)
                    batch_buffer.clear()
            
            self.stats.end_timing()
            logger.info("Log generation completed successfully")
            
            # çµ±è¨ˆå‡ºåŠ›
            self.stats.print_summary(self.config)
            
            print("\nâœ… Generation completed successfully!")
            print(f"ğŸ“ Output: {self.config.output_file}")
        
        except KeyboardInterrupt:
            logger.warning("Generation interrupted by user")
            self.stats.end_timing()
            
            # éƒ¨åˆ†çš„ãªçµæœã‚’ä¿å­˜
            if batch_buffer:
                self._save_partial_results(batch_buffer, "interrupted")
            
            print("\n\nâš ï¸  Generation interrupted by user")
            self.stats.print_summary(self.config)
            
            sys.exit(130)
        
        except IOError as e:
            logger.error(f"File I/O error: {e}")
            
            # éƒ¨åˆ†çš„ãªçµæœã‚’ä¿å­˜
            if batch_buffer:
                self._save_partial_results(batch_buffer, "io_error")
            
            print(f"\nâŒ File I/O error: {e}")
            sys.exit(1)
        
        except Exception as e:
            logger.exception(f"Unexpected error: {e}")
            
            # éƒ¨åˆ†çš„ãªçµæœã‚’ä¿å­˜
            if batch_buffer:
                self._save_partial_results(batch_buffer, "error")
            
            print(f"\nâŒ Unexpected error: {e}")
            print("Check logs for details")
            sys.exit(1)
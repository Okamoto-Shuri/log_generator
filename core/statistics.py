"""
Core Statistics Module - Part 3-3 Integration (çµ±è¨ˆéƒ¨åˆ†)

çµ±è¨ˆåé›†ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œè¨¼
"""

import json
from typing import Dict, Any, Set
from collections import defaultdict
from datetime import datetime

# core.configã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .config import logger, GeneratorConfig, SCENARIO_META


# ==================== çµ±è¨ˆç®¡ç† ====================

class StatisticsCollector:
    """ç”Ÿæˆçµ±è¨ˆã‚’åé›†ãƒ»ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.scenario_counts: Dict[str, int] = defaultdict(int)
        self.category_counts: Dict[str, int] = defaultdict(int)
        self.severity_counts: Dict[str, int] = defaultdict(int)
        self.total_logs: int = 0
        self.total_events: int = 0
        self.start_time: Optional[datetime] = None
        self.end_time: Optional[datetime] = None
    
    def record_scenario(self, scenario_code: str, log_count: int) -> None:
        """ã‚·ãƒŠãƒªã‚ªç”Ÿæˆã‚’è¨˜éŒ²"""
        self.scenario_counts[scenario_code] += 1
        self.total_logs += log_count
        self.total_events += 1
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
        if scenario_code != "normal":
            meta = SCENARIO_META.get(scenario_code)
            if meta:
                self.category_counts[meta.category] += 1
                self.severity_counts[meta.severity] += 1
    
    def start_timing(self) -> None:
        """è¨ˆæ¸¬é–‹å§‹"""
        self.start_time = datetime.now()
    
    def end_timing(self) -> None:
        """è¨ˆæ¸¬çµ‚äº†"""
        self.end_time = datetime.now()
    
    def get_elapsed_time(self) -> float:
        """çµŒéæ™‚é–“ã‚’ç§’ã§å–å¾—"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return 0.0
    
    def print_summary(self, config: GeneratorConfig) -> None:
        """çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›"""
        print("\n" + "=" * 70)
        print("=" * 25 + " GENERATION SUMMARY " + "=" * 25)
        print("=" * 70)
        
        # åŸºæœ¬æƒ…å ±
        print(f"\nğŸ“Š Basic Statistics:")
        print(f"  Total Events Generated:  {self.total_events:,}")
        print(f"  Total Log Records:       {self.total_logs:,}")
        print(f"  Average Logs per Event:  {self.total_logs / max(self.total_events, 1):.2f}")
        print(f"  Output File:             {config.output_file}")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
        elapsed = self.get_elapsed_time()
        if elapsed > 0:
            print(f"\nâ±ï¸  Performance:")
            print(f"  Elapsed Time:            {elapsed:.2f} seconds")
            print(f"  Events per Second:       {self.total_events / elapsed:.2f}")
            print(f"  Logs per Second:         {self.total_logs / elapsed:.2f}")
        
        # æ­£å¸¸/ç•°å¸¸ã®æ¯”ç‡
        normal_count = self.scenario_counts.get("normal", 0)
        abnormal_count = self.total_events - normal_count
        
        print(f"\nğŸ“ˆ Event Distribution:")
        print(f"  Normal Events:           {normal_count:>6,} ({normal_count/max(self.total_events,1)*100:>5.1f}%)")
        print(f"  Abnormal Events:         {abnormal_count:>6,} ({abnormal_count/max(self.total_events,1)*100:>5.1f}%)")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        if self.category_counts:
            print(f"\nğŸ·ï¸  Abnormal Events by Category:")
            sorted_categories = sorted(
                self.category_counts.items(),
                key=lambda x: x[1],
                reverse=True
            )
            for category, count in sorted_categories:
                pct = count / max(abnormal_count, 1) * 100
                print(f"  {category:<20} {count:>6,} ({pct:>5.1f}% of abnormal)")
        
        # é‡å¤§åº¦åˆ¥çµ±è¨ˆ
        if self.severity_counts:
            print(f"\nâš ï¸  Abnormal Events by Severity:")
            severity_order = ["critical", "fatal", "error", "warning", "info"]
            for severity in severity_order:
                if severity in self.severity_counts:
                    count = self.severity_counts[severity]
                    pct = count / max(abnormal_count, 1) * 100
                    print(f"  {severity.upper():<20} {count:>6,} ({pct:>5.1f}% of abnormal)")
        
        # ã‚·ãƒŠãƒªã‚ªåˆ¥çµ±è¨ˆï¼ˆä¸Šä½10ä»¶ï¼‰
        print(f"\nğŸ“‹ Top 10 Scenario Frequencies:")
        sorted_scenarios = sorted(
            self.scenario_counts.items(),
            key=lambda x: x[1],
            reverse=True
        )
        for idx, (code, count) in enumerate(sorted_scenarios[:10], 1):
            if code == "normal":
                cause = "Normal Operation"
                category = "normal"
            else:
                meta = SCENARIO_META.get(code)
                cause = meta.cause if meta else "unknown"
                category = meta.category if meta else "unknown"
            
            pct = count / max(self.total_events, 1) * 100
            print(f"  {idx:2}. [{code}] {cause:<35} {count:>5,} ({pct:>5.1f}%)")
            print(f"      Category: {category}")
        
        print("\n" + "=" * 70)


# ==================== æ¤œè¨¼ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ====================

class DatasetValidator:
    """ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¤œè¨¼ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def validate_file(file_path: str) -> Dict[str, Any]:
        """
        JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼
        
        Args:
            file_path: æ¤œè¨¼ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            æ¤œè¨¼çµæœã®è¾æ›¸
        """
        logger.info(f"Validating dataset: {file_path}")
        
        results = {
            "valid": True,
            "total_lines": 0,
            "invalid_lines": [],
            "missing_fields": defaultdict(int),
            "unique_traces": set(),
            "unique_correlations": set(),
            "timestamp_errors": 0
        }
        
        required_fields = [
            "timestamp", "service", "host", "level",
            "message", "metrics", "label", "message_vector"
        ]
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                prev_timestamp = None
                
                for line_num, line in enumerate(f, 1):
                    results["total_lines"] += 1
                    
                    try:
                        record = json.loads(line.strip())
                        
                        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
                        for field in required_fields:
                            if field not in record:
                                results["missing_fields"][field] += 1
                                results["valid"] = False
                        
                        # trace_idã¨correlation_idã®åé›†
                        if record.get("trace_id"):
                            results["unique_traces"].add(record["trace_id"])
                        if record.get("correlation_id"):
                            results["unique_correlations"].add(record["correlation_id"])
                        
                        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®é †åºãƒã‚§ãƒƒã‚¯
                        current_timestamp = record.get("timestamp")
                        if prev_timestamp and current_timestamp:
                            if current_timestamp < prev_timestamp:
                                results["timestamp_errors"] += 1
                        prev_timestamp = current_timestamp
                    
                    except json.JSONDecodeError:
                        results["invalid_lines"].append(line_num)
                        results["valid"] = False
            
            # çµ±è¨ˆå¤‰æ›
            results["unique_traces"] = len(results["unique_traces"])
            results["unique_correlations"] = len(results["unique_correlations"])
            results["missing_fields"] = dict(results["missing_fields"])
            
            logger.info(
                f"Validation completed: "
                f"{'PASS' if results['valid'] else 'FAIL'}"
            )
            
            return results
        
        except FileNotFoundError:
            logger.error(f"File not found: {file_path}")
            return {"valid": False, "error": "File not found"}
        except Exception as e:
            logger.error(f"Validation error: {e}")
            return {"valid": False, "error": str(e)}
    
    @staticmethod
    def print_validation_results(results: Dict) -> None:
        """æ¤œè¨¼çµæœã‚’å‡ºåŠ›"""
        print("\n" + "=" * 70)
        print("=" * 25 + " VALIDATION RESULTS " + "=" * 26)
        print("=" * 70)
        
        if "error" in results:
            print(f"\nâŒ Validation Error: {results['error']}")
            return
        
        status = "âœ… PASS" if results["valid"] else "âŒ FAIL"
        print(f"\nStatus: {status}")
        print(f"Total Lines: {results['total_lines']:,}")
        
        if results["invalid_lines"]:
            print(f"\nâš ï¸  Invalid JSON Lines: {len(results['invalid_lines'])}")
            print(f"   Line numbers: {results['invalid_lines'][:10]}")
            if len(results['invalid_lines']) > 10:
                print(f"   ... and {len(results['invalid_lines']) - 10} more")
        
        if results["missing_fields"]:
            print(f"\nâš ï¸  Missing Fields:")
            for field, count in results["missing_fields"].items():
                print(f"   {field}: {count} occurrences")
        
        print(f"\nğŸ“Š Statistics:")
        print(f"   Unique Trace IDs:       {results['unique_traces']:,}")
        print(f"   Unique Correlation IDs: {results['unique_correlations']:,}")
        
        if results["timestamp_errors"] > 0:
            print(f"\nâš ï¸  Timestamp Ordering Errors: {results['timestamp_errors']}")
        
        print("\n" + "=" * 70)